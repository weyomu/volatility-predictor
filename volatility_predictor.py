# -*- coding: utf-8 -*-
"""
æ¯”ç‰¹å¸æ³¢åŠ¨é¢„è­¦ç³»ç»Ÿ
ä¸“æ³¨äºé¢„æµ‹æœªæ¥1-3å¤©æ˜¯å¦ä¼šå‡ºç°å¤§æ¶¨å¤§è·Œ
ä¸æ¶‰åŠäº¤æ˜“ï¼Œåªåšé£é™©é¢„è­¦
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve, precision_recall_curve, auc
from sklearn.utils.class_weight import compute_class_weight
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.optimizers import Adam
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡æ˜¾ç¤º
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False
plt.rcParams['figure.figsize'] = (14, 8)

# =============================================================================
# 1. è¾…åŠ©å‡½æ•°ï¼ˆä¸åŸå§‹ä»£ç ç›¸åŒï¼‰
# =============================================================================

def convert_volume(vol_str):
    """è½¬æ¢äº¤æ˜“é‡æ ¼å¼"""
    if isinstance(vol_str, str):
        vol_str = vol_str.replace(',', '')
        if 'B' in vol_str:
            return float(vol_str.replace('B', '')) * 1e9
        elif 'M' in vol_str:
            return float(vol_str.replace('M', '')) * 1e6
        elif 'K' in vol_str:
            return float(vol_str.replace('K', '')) * 1e3
        else:
            return float(vol_str)
    return vol_str

def convert_change(change_str):
    """è½¬æ¢æ¶¨è·Œå¹…æ ¼å¼"""
    if isinstance(change_str, str):
        clean_str = change_str.replace('%', '').replace(',', '').strip()
        if clean_str == '-' or clean_str == 'nan':
            return 0.0
        return float(clean_str) / 100
    return change_str

def calculate_rsi(series, period=14):
    """è®¡ç®—RSIæŒ‡æ ‡"""
    series = pd.to_numeric(series, errors='coerce')
    delta = series.diff(1)
    gain = delta.where(delta > 0, 0)
    loss = -delta.where(delta < 0, 0)
    
    avg_gain = gain.ewm(alpha=1/period, min_periods=period).mean()
    avg_loss = loss.ewm(alpha=1/period, min_periods=period).mean()
    
    rs = avg_gain / avg_loss
    rsi = 100 - (100 / (1 + rs))
    return rsi.replace([np.inf, -np.inf], np.nan).fillna(50)

def calculate_atr(data, period=14):
    """è®¡ç®—ATRæŒ‡æ ‡"""
    high = data['é«˜']
    low = data['ä½']
    close = data['æ”¶ç›˜']
    
    tr1 = high - low
    tr2 = abs(high - close.shift(1))
    tr3 = abs(low - close.shift(1))
    
    tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)
    atr = tr.ewm(alpha=1/period, min_periods=period).mean()
    return atr

# =============================================================================
# 2. æ•°æ®åŠ è½½å’Œç‰¹å¾å·¥ç¨‹ï¼ˆä¸åŸå§‹ä»£ç ç›¸åŒï¼‰
# =============================================================================

def load_and_engineer_features(filepath):
    """åŠ è½½æ•°æ®å¹¶è¿›è¡Œç‰¹å¾å·¥ç¨‹"""
    # åŠ è½½æ•°æ®
    print(f"  â†’ è¯»å–CSVæ–‡ä»¶: {filepath}")
    df = pd.read_csv(filepath, parse_dates=['æ—¥æœŸ'])
    print(f"  â†’ åŸå§‹æ•°æ®è¡Œæ•°: {len(df)}")
    df = df[df['æ—¥æœŸ'].dt.year > 2000]
    print(f"  â†’ è¿‡æ»¤2000å¹´åæ•°æ®: {len(df)} è¡Œ")
    
    # è½¬æ¢æ ¼å¼
    print(f"  â†’ è½¬æ¢äº¤æ˜“é‡æ ¼å¼...")
    df['äº¤æ˜“é‡'] = df['äº¤æ˜“é‡'].apply(convert_volume)
    print(f"  â†’ è½¬æ¢æ¶¨è·Œå¹…æ ¼å¼...")
    df['æ¶¨è·Œå¹…'] = df['æ¶¨è·Œå¹…'].apply(convert_change)
    
    # ç¡®ä¿ä»·æ ¼åˆ—æ˜¯æ•°å€¼ç±»å‹
    print(f"  â†’ è½¬æ¢ä»·æ ¼åˆ—ä¸ºæ•°å€¼ç±»å‹...")
    for col in ['æ”¶ç›˜', 'å¼€ç›˜', 'é«˜', 'ä½']:
        df[col] = df[col].astype(str).str.replace(',', '').str.replace('$', '').str.replace('Â¥', '')
        df[col] = pd.to_numeric(df[col], errors='coerce')
    
    # æ’åºå’Œè®¾ç½®ç´¢å¼•
    print(f"  â†’ æŒ‰æ—¥æœŸæ’åºå¹¶è®¾ç½®ç´¢å¼•...")
    df.sort_values('æ—¥æœŸ', inplace=True)
    df.set_index('æ—¥æœŸ', inplace=True)
    df.fillna(method='ffill', inplace=True)
    df.replace([np.inf, -np.inf], np.nan, inplace=True)
    df.fillna(method='ffill', inplace=True)
    
    # ç‰¹å¾å·¥ç¨‹
    print(f"  â†’ è®¡ç®—åŸºç¡€ç‰¹å¾...")
    df['æ—¥å˜åŒ–'] = df['æ”¶ç›˜'].diff()
    df['å¼€ç›˜æ”¶ç›˜å·®'] = df['æ”¶ç›˜'] - df['å¼€ç›˜']
    df['é«˜ä½å·®'] = df['é«˜'] - df['ä½']
    
    print(f"  â†’ è®¡ç®—ç§»åŠ¨å¹³å‡çº¿ (SMA 7å¤©, 30å¤©)...")
    df['SMA_7'] = df['æ”¶ç›˜'].rolling(window=7, min_periods=1).mean()
    df['SMA_30'] = df['æ”¶ç›˜'].rolling(window=30, min_periods=1).mean()
    
    print(f"  â†’ è®¡ç®—æŒ‡æ•°ç§»åŠ¨å¹³å‡çº¿å’ŒMACD...")
    df['EMA_12'] = df['æ”¶ç›˜'].ewm(span=12, adjust=False, min_periods=1).mean()
    df['EMA_26'] = df['æ”¶ç›˜'].ewm(span=26, adjust=False, min_periods=1).mean()
    df['MACD'] = df['EMA_12'] - df['EMA_26']
    df['ä¿¡å·çº¿'] = df['MACD'].ewm(span=9, adjust=False).mean()
    df['MACD_Hist'] = df['MACD'] - df['ä¿¡å·çº¿']
    
    print(f"  â†’ è®¡ç®—æ³¢åŠ¨ç‡å’ŒRSI...")
    df['æ³¢åŠ¨ç‡'] = df['æ”¶ç›˜'].rolling(window=60, min_periods=1).std()
    df['RSI'] = calculate_rsi(df['æ”¶ç›˜'], 14)
    
    print(f"  â†’ è®¡ç®—å¸ƒæ—å¸¦...")
    df['ä¸­è½¨'] = df['æ”¶ç›˜'].rolling(window=20).mean()
    df['ä¸Šè½¨'] = df['ä¸­è½¨'] + 2 * df['æ”¶ç›˜'].rolling(window=20).std()
    df['ä¸‹è½¨'] = df['ä¸­è½¨'] - 2 * df['æ”¶ç›˜'].rolling(window=20).std()
    
    print(f"  â†’ è®¡ç®—ATRæŒ‡æ ‡...")
    df['ATR'] = calculate_atr(df, 14)
    
    # æ»åç‰¹å¾
    print(f"  â†’ åˆ›å»º60å¤©æ»åç‰¹å¾...")
    for i in range(1, 61):
        df[f'æ»å_{i}'] = df['æ”¶ç›˜'].shift(i)
    
    print(f"  â†’ åˆ é™¤ç¼ºå¤±å€¼...")
    print(f"  â†’ åˆ é™¤å‰æ•°æ®é‡: {len(df)}")
    df.dropna(inplace=True)
    print(f"  â†’ åˆ é™¤åæ•°æ®é‡: {len(df)}")
    print(f"  â†’ æœ€ç»ˆç‰¹å¾æ•°é‡: {len(df.columns)}")
    return df

# =============================================================================
# 3. ğŸ†• å…³é”®æ”¹è¿›ï¼šå®šä¹‰æ³¢åŠ¨æ ‡ç­¾ï¼ˆåŸå§‹ä»£ç æ²¡æœ‰è¿™ä¸ªï¼ï¼‰
# =============================================================================

def create_volatility_labels(df, days_ahead=1, threshold=0.03):
    """
    åˆ›å»ºæ³¢åŠ¨æ ‡ç­¾
    
    æ ¸å¿ƒé€»è¾‘ï¼š
    - å¦‚æœæœªæ¥Nå¤©æ¶¨è·Œå¹…ç»å¯¹å€¼ > thresholdï¼Œæ ‡è®°ä¸º"é«˜æ³¢åŠ¨"
    - å¦åˆ™æ ‡è®°ä¸º"ä½æ³¢åŠ¨"
    
    å‚æ•°:
        days_ahead: é¢„æµ‹æœªæ¥å¤šå°‘å¤©ï¼ˆ1-3å¤©ï¼‰
        threshold: æ³¢åŠ¨é˜ˆå€¼ï¼ˆ3% = 0.03ï¼‰
    
    è¿”å›:
        0 = ä½æ³¢åŠ¨ï¼ˆæ­£å¸¸ï¼‰
        1 = é«˜æ³¢åŠ¨ï¼ˆå¤§æ¶¨å¤§è·Œï¼‰
    """
    print(f"  â†’ è®¡ç®—æœªæ¥{days_ahead}å¤©çš„æ”¶ç›Šç‡...")
    future_returns = []
    
    for i in range(days_ahead):
        future_price = df['æ”¶ç›˜'].shift(-(i+1))
        ret = (future_price - df['æ”¶ç›˜']) / df['æ”¶ç›˜']
        future_returns.append(ret.abs())
    
    # å–æœªæ¥Nå¤©çš„æœ€å¤§æ¶¨è·Œå¹…
    print(f"  â†’ è®¡ç®—æœªæ¥{days_ahead}å¤©çš„æœ€å¤§æ¶¨è·Œå¹…...")
    max_future_change = pd.concat(future_returns, axis=1).max(axis=1)
    
    # æ ‡è®°é«˜æ³¢åŠ¨
    print(f"  â†’ æ ‡è®°é«˜æ³¢åŠ¨äº‹ä»¶ (é˜ˆå€¼: {threshold*100}%)...")
    labels = (max_future_change > threshold).astype(int)
    
    print(f"  â†’ æœ€å¤§æ¶¨è·Œå¹…ç»Ÿè®¡: å‡å€¼={max_future_change.mean()*100:.2f}%, æœ€å¤§={max_future_change.max()*100:.2f}%")
    
    return labels, max_future_change

# =============================================================================
# 4. ğŸ†• å…³é”®æ”¹è¿›ï¼šæ­£ç¡®çš„æ•°æ®åˆ’åˆ†ï¼ˆé¿å…æ•°æ®æ³„æ¼ï¼‰
# =============================================================================

def prepare_data_for_classification(df, labels, time_steps=60, train_ratio=0.8):
    """
    å‡†å¤‡åˆ†ç±»æ•°æ®ï¼ˆé¢„æµ‹é«˜æ³¢åŠ¨/ä½æ³¢åŠ¨ï¼‰
    
    ğŸ”‘ å…³é”®æ”¹è¿›ï¼šå…ˆåˆ’åˆ†å†æ ‡å‡†åŒ–ï¼Œé¿å…æ•°æ®æ³„æ¼ï¼
    """
    print(f"  â†’ æå–ç‰¹å¾åˆ—...")
    feature_cols = [col for col in df.columns if not col.startswith('ç›®æ ‡_')]
    print(f"  â†’ ç‰¹å¾åˆ—æ•°é‡: {len(feature_cols)}")
    
    # å…ˆæŒ‰æ—¶é—´åˆ’åˆ†
    print(f"  â†’ æŒ‰æ—¶é—´åˆ’åˆ†æ•°æ®é›† (è®­ç»ƒé›†æ¯”ä¾‹: {train_ratio*100}%)...")
    split_idx = int(len(df) * train_ratio)
    train_df = df.iloc[:split_idx].copy()
    test_df = df.iloc[split_idx:].copy()
    train_labels = labels[:split_idx]
    test_labels = labels[split_idx:]
    
    print(f"è®­ç»ƒé›†æ—¶é—´: {train_df.index[0]} åˆ° {train_df.index[-1]}")
    print(f"æµ‹è¯•é›†æ—¶é—´: {test_df.index[0]} åˆ° {test_df.index[-1]}")
    
    # âœ… åªç”¨è®­ç»ƒé›†æ‹Ÿåˆscaler
    print(f"  â†’ ä½¿ç”¨è®­ç»ƒé›†æ‹ŸåˆMinMaxScaler...")
    scaler = MinMaxScaler()
    scaler.fit(train_df[feature_cols])
    
    print(f"  â†’ æ ‡å‡†åŒ–è®­ç»ƒé›†æ•°æ®...")
    train_scaled = scaler.transform(train_df[feature_cols])
    print(f"  â†’ æ ‡å‡†åŒ–æµ‹è¯•é›†æ•°æ®...")
    test_scaled = scaler.transform(test_df[feature_cols])
    
    # åˆ›å»ºæ—¶é—´åºåˆ—æ•°æ®é›†
    print(f"  â†’ åˆ›å»ºæ—¶é—´åºåˆ—æ ·æœ¬ (æ—¶é—´æ­¥é•¿: {time_steps})...")
    def create_sequences(data, labels, time_steps):
        X, y = [], []
        for i in range(len(data) - time_steps):
            X.append(data[i:(i + time_steps), :])
            y.append(labels[i + time_steps])
        return np.array(X), np.array(y)
    
    print(f"  â†’ ç”Ÿæˆè®­ç»ƒåºåˆ—...")
    X_train, y_train = create_sequences(train_scaled, train_labels, time_steps)
    print(f"  â†’ ç”Ÿæˆæµ‹è¯•åºåˆ—...")
    X_test, y_test = create_sequences(test_scaled, test_labels, time_steps)
    
    return {
        'X_train': X_train,
        'y_train': y_train,
        'X_test': X_test,
        'y_test': y_test,
        'test_dates': test_df.index[time_steps:],
        'test_prices': test_df['æ”¶ç›˜'].values[time_steps:],
        'scaler': scaler,
        'feature_cols': feature_cols
    }

# =============================================================================
# 5. æ„å»ºåˆ†ç±»æ¨¡å‹ï¼ˆé¢„æµ‹é«˜æ³¢åŠ¨/ä½æ³¢åŠ¨ï¼‰
# =============================================================================

def create_volatility_classifier(input_shape, dropout_rate=0.2, learning_rate=0.0005):
    """åˆ›å»ºæ³¢åŠ¨åˆ†ç±»æ¨¡å‹ - ä¼˜åŒ–ç‰ˆ"""
    from tensorflow.keras.layers import BatchNormalization
    
    print(f"  â†’ åˆ›å»ºä¼˜åŒ–å‹LSTM+MLPæ¨¡å‹...")
    print(f"  â†’ è¾“å…¥å½¢çŠ¶: {input_shape}")
    
    model = Sequential([
        # ç¬¬ä¸€å±‚LSTM - 128å•å…ƒ
        LSTM(128, return_sequences=True, input_shape=input_shape, recurrent_dropout=0.1),
        BatchNormalization(),
        Dropout(dropout_rate),
        
        # ç¬¬äºŒå±‚LSTM - 64å•å…ƒ
        LSTM(64, return_sequences=False, recurrent_dropout=0.1),
        BatchNormalization(),
        Dropout(dropout_rate),
        
        # MLPéƒ¨åˆ†
        Dense(128, activation='relu'),
        BatchNormalization(),
        Dropout(dropout_rate),
        
        Dense(64, activation='relu'),
        BatchNormalization(),
        Dropout(dropout_rate),
        
        Dense(32, activation='relu'),
        Dropout(dropout_rate),
        
        # è¾“å‡ºå±‚
        Dense(1, activation='sigmoid')
    ])
    
    print(f"  â†’ æ¨¡å‹ç»“æ„ (æ·»åŠ BatchNormalizationç¨³å®šè®­ç»ƒ):")
    print(f"     â”œâ”€ LSTMå±‚1: 128å•å…ƒ (recurrent_dropout=0.1)")
    print(f"     â”œâ”€ BatchNorm + Dropout: {dropout_rate*100:.0f}%")
    print(f"     â”œâ”€ LSTMå±‚2: 64å•å…ƒ (recurrent_dropout=0.1)")
    print(f"     â”œâ”€ BatchNorm + Dropout: {dropout_rate*100:.0f}%")
    print(f"     â”œâ”€ Denseå±‚1: 128å•å…ƒ (ReLU)")
    print(f"     â”œâ”€ BatchNorm + Dropout: {dropout_rate*100:.0f}%")
    print(f"     â”œâ”€ Denseå±‚2: 64å•å…ƒ (ReLU)")
    print(f"     â”œâ”€ BatchNorm + Dropout: {dropout_rate*100:.0f}%")
    print(f"     â”œâ”€ Denseå±‚3: 32å•å…ƒ (ReLU)")
    print(f"     â”œâ”€ Dropout: {dropout_rate*100:.0f}%")
    print(f"     â””â”€ è¾“å‡ºå±‚: 1å•å…ƒ (Sigmoid)")
    
    model.compile(
        optimizer=Adam(learning_rate=learning_rate, clipnorm=1.0),  # æ·»åŠ æ¢¯åº¦è£å‰ª
        loss='binary_crossentropy',
        metrics=['accuracy']
    )
    
    print(f"  â†’ ä¼˜åŒ–å™¨: Adam (learning_rate={learning_rate}, clipnorm=1.0)")
    print(f"  â†’ æŸå¤±å‡½æ•°: binary_crossentropy")
    print(f"  â†’ è¯„ä¼°æŒ‡æ ‡: accuracy")
    
    total_params = model.count_params()
    print(f"  â†’ æ€»å‚æ•°é‡: {total_params:,}")
    
    return model

# =============================================================================
# 6. ä¸»å‡½æ•°
# =============================================================================

def main():
    print("="*70)
    print("æ¯”ç‰¹å¸æ³¢åŠ¨é¢„è­¦ç³»ç»Ÿ")
    print("é¢„æµ‹æœªæ¥1-3å¤©æ˜¯å¦ä¼šå‡ºç°å¤§æ¶¨å¤§è·Œ")
    print("="*70)
    
    # å¯è°ƒå‚æ•°
    DAYS_AHEAD = 1        # é¢„æµ‹æœªæ¥å‡ å¤©ï¼ˆ1-3å¤©ï¼‰
    THRESHOLD = 0.03      # æ³¢åŠ¨é˜ˆå€¼ï¼š3%
    TIME_STEPS = 60       # ä½¿ç”¨è¿‡å»60å¤©æ•°æ®
    TRAIN_RATIO = 0.8     # 80%è®­ç»ƒï¼Œ20%æµ‹è¯•
    DROPOUT_RATE = 0.2    # Dropoutæ¯”ç‡ï¼ˆå…ˆç”¨0.2ï¼Œå¤ªä½å¯èƒ½å¯¼è‡´ä¸ç¨³å®šï¼‰
    EPOCHS = 100          # æœ€å¤§è®­ç»ƒè½®æ•°
    BATCH_SIZE = 16       # æ‰¹æ¬¡å¤§å°ï¼ˆå‡å°ï¼Œè®©æ¢¯åº¦æ›´æ–°æ›´é¢‘ç¹ï¼‰
    LEARNING_RATE = 0.0005 # å­¦ä¹ ç‡ï¼ˆé™ä½ï¼Œæ›´ç¨³å®šï¼‰
    USE_CLASS_WEIGHT = False  # æš‚æ—¶å…³é—­ç±»åˆ«æƒé‡ï¼Œçœ‹æ˜¯å¦å½±å“è®­ç»ƒ
    
    print(f"\nå‚æ•°è®¾ç½®:")
    print(f"- é¢„æµ‹æ—¶é•¿: æœªæ¥{DAYS_AHEAD}å¤©")
    print(f"- æ³¢åŠ¨é˜ˆå€¼: {THRESHOLD*100}% (è¶…è¿‡æ­¤å€¼è§†ä¸ºé«˜æ³¢åŠ¨)")
    print(f"- è®­ç»ƒé›†æ¯”ä¾‹: {TRAIN_RATIO*100}%")
    print(f"- Dropoutæ¯”ç‡: {DROPOUT_RATE*100}%")
    print(f"- å­¦ä¹ ç‡: {LEARNING_RATE}")
    print(f"- æ‰¹æ¬¡å¤§å°: {BATCH_SIZE}")
    print(f"- ç±»åˆ«æƒé‡: {'å¯ç”¨' if USE_CLASS_WEIGHT else 'ç¦ç”¨'}")
    
    # 1. åŠ è½½æ•°æ®
    print("\n[1/5] åŠ è½½æ•°æ®å’Œç‰¹å¾å·¥ç¨‹...")
    df = load_and_engineer_features('æ¯”ç‰¹å¸å†å²æ•°æ®2.csv')
    print(f"æ•°æ®æ—¶é—´èŒƒå›´: {df.index[0]} åˆ° {df.index[-1]}")
    print(f"æ€»æ•°æ®é‡: {len(df)} å¤©")
    print(f"ä»·æ ¼ç»Ÿè®¡: æœ€ä½=${df['æ”¶ç›˜'].min():,.2f}, æœ€é«˜=${df['æ”¶ç›˜'].max():,.2f}, å¹³å‡=${df['æ”¶ç›˜'].mean():,.2f}")
    
    # 2. åˆ›å»ºæ³¢åŠ¨æ ‡ç­¾
    print(f"\n[2/5] åˆ›å»ºæ³¢åŠ¨æ ‡ç­¾ï¼ˆé˜ˆå€¼ï¼š{THRESHOLD*100}%ï¼‰...")
    labels, future_changes = create_volatility_labels(df, days_ahead=DAYS_AHEAD, threshold=THRESHOLD)
    
    # ç»Ÿè®¡æ ‡ç­¾åˆ†å¸ƒ
    high_vol_count = labels.sum()
    low_vol_count = len(labels) - high_vol_count
    print(f"é«˜æ³¢åŠ¨å¤©æ•°: {high_vol_count} ({high_vol_count/len(labels)*100:.1f}%)")
    print(f"ä½æ³¢åŠ¨å¤©æ•°: {low_vol_count} ({low_vol_count/len(labels)*100:.1f}%)")
    print(f"ç±»åˆ«å¹³è¡¡æ¯”: 1:{low_vol_count/high_vol_count:.2f} (é«˜æ³¢åŠ¨:ä½æ³¢åŠ¨)")
    
    # 3. å‡†å¤‡æ•°æ®
    print("\n[3/5] å‡†å¤‡è®­ç»ƒå’Œæµ‹è¯•æ•°æ®ï¼ˆé¿å…æ•°æ®æ³„æ¼ï¼‰...")
    data = prepare_data_for_classification(df, labels, time_steps=TIME_STEPS, train_ratio=TRAIN_RATIO)
    print(f"è®­ç»ƒæ ·æœ¬æ•°: {len(data['X_train'])} (é«˜æ³¢åŠ¨: {data['y_train'].sum()}, æ¯”ä¾‹: {data['y_train'].sum()/len(data['y_train'])*100:.1f}%)")
    print(f"æµ‹è¯•æ ·æœ¬æ•°: {len(data['X_test'])} (é«˜æ³¢åŠ¨: {data['y_test'].sum()}, æ¯”ä¾‹: {data['y_test'].sum()/len(data['y_test'])*100:.1f}%)")
    print(f"è¾“å…¥ç‰¹å¾ç»´åº¦: {data['X_train'].shape}")
    
    # 3.5 è®¡ç®—ç±»åˆ«æƒé‡ï¼ˆè§£å†³ä¸å¹³è¡¡é—®é¢˜ï¼‰
    print("\n[3.5/5] è®¡ç®—ç±»åˆ«æƒé‡ï¼ˆè§£å†³ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ï¼‰...")
    class_weights = compute_class_weight(
        'balanced',
        classes=np.unique(data['y_train']),
        y=data['y_train']
    )
    class_weight_dict = {0: class_weights[0], 1: class_weights[1]}
    print(f"  â†’ ä½æ³¢åŠ¨ç±»åˆ«æƒé‡: {class_weights[0]:.4f}")
    print(f"  â†’ é«˜æ³¢åŠ¨ç±»åˆ«æƒé‡: {class_weights[1]:.4f}")
    print(f"  â†’ æƒé‡æ¯”ä¾‹: 1:{class_weights[1]/class_weights[0]:.2f}")
    print(f"  â†’ è¯´æ˜: é«˜æ³¢åŠ¨æ ·æœ¬å°†è·å¾—{class_weights[1]/class_weights[0]:.2f}å€çš„å…³æ³¨åº¦")
    
    # 4. è®­ç»ƒæ¨¡å‹
    print("\n[4/5] è®­ç»ƒæ³¢åŠ¨é¢„æµ‹æ¨¡å‹...")
    model = create_volatility_classifier(
        input_shape=(data['X_train'].shape[1], data['X_train'].shape[2]),
        dropout_rate=DROPOUT_RATE,
        learning_rate=LEARNING_RATE
    )
    
    print(f"  â†’ è®¾ç½®æ—©åœæœºåˆ¶ (patience=20, monitor=val_loss)...")
    early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)
    
    if USE_CLASS_WEIGHT:
        print(f"  â†’ å¼€å§‹è®­ç»ƒï¼ˆä½¿ç”¨ç±»åˆ«æƒé‡ï¼‰...")
        print(f"  â†’ åº”ç”¨ç±»åˆ«æƒé‡: ä½æ³¢åŠ¨={class_weight_dict[0]:.4f}, é«˜æ³¢åŠ¨={class_weight_dict[1]:.4f}")
    else:
        print(f"  â†’ å¼€å§‹è®­ç»ƒï¼ˆä¸ä½¿ç”¨ç±»åˆ«æƒé‡ï¼‰...")
        print(f"  â†’ æ³¨æ„: ç±»åˆ«æƒé‡å·²ç¦ç”¨ï¼Œä½¿ç”¨å‡è¡¡é‡‡æ ·")
    
    print(f"  â†’ è®­ç»ƒæ ·æœ¬: {len(data['X_train'])}")
    print(f"  â†’ éªŒè¯æ ·æœ¬: {int(len(data['X_train']) * 0.2)}")
    print(f"  â†’ æœ€å¤§è½®æ•°: {EPOCHS}")
    print(f"  â†’ æ‰¹æ¬¡å¤§å°: {BATCH_SIZE}")
    print(f"  â†’ æ˜¾ç¤ºè®­ç»ƒè¿›åº¦...")
    print()
    
    history = model.fit(
        data['X_train'], data['y_train'],
        validation_split=0.2,
        epochs=EPOCHS,
        batch_size=BATCH_SIZE,
        callbacks=[early_stop],
        class_weight=class_weight_dict if USE_CLASS_WEIGHT else None,
        verbose=1  # æ˜¾ç¤ºè®­ç»ƒè¿›åº¦
    )
    
    print(f"\n  â†’ å®é™…è®­ç»ƒè½®æ•°: {len(history.history['loss'])}")
    print(f"  â†’ æœ€ç»ˆè®­ç»ƒæŸå¤±: {history.history['loss'][-1]:.4f}")
    print(f"  â†’ æœ€ç»ˆéªŒè¯æŸå¤±: {history.history['val_loss'][-1]:.4f}")
    print(f"  â†’ æœ€ç»ˆè®­ç»ƒå‡†ç¡®ç‡: {history.history['accuracy'][-1]*100:.2f}%")
    print(f"  â†’ æœ€ç»ˆéªŒè¯å‡†ç¡®ç‡: {history.history['val_accuracy'][-1]*100:.2f}%")
    
    # æ˜¾ç¤ºè®­ç»ƒå†å²æ‘˜è¦
    print(f"\n  è®­ç»ƒå†å²æ‘˜è¦:")
    print(f"  â†’ æœ€ä½³è®­ç»ƒå‡†ç¡®ç‡: {max(history.history['accuracy'])*100:.2f}% (ç¬¬{history.history['accuracy'].index(max(history.history['accuracy']))+1}è½®)")
    print(f"  â†’ æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {max(history.history['val_accuracy'])*100:.2f}% (ç¬¬{history.history['val_accuracy'].index(max(history.history['val_accuracy']))+1}è½®)")
    print(f"  â†’ æœ€ä½è®­ç»ƒæŸå¤±: {min(history.history['loss']):.4f} (ç¬¬{history.history['loss'].index(min(history.history['loss']))+1}è½®)")
    print(f"  â†’ æœ€ä½éªŒè¯æŸå¤±: {min(history.history['val_loss']):.4f} (ç¬¬{history.history['val_loss'].index(min(history.history['val_loss']))+1}è½®)")
    print("æ¨¡å‹è®­ç»ƒå®Œæˆ!")
    
    # 5. æµ‹è¯•é›†è¯„ä¼°
    print("\n[5/5] åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°...")
    print(f"  â†’ å¯¹æµ‹è¯•é›†è¿›è¡Œé¢„æµ‹...")
    y_pred_prob = model.predict(data['X_test'], verbose=0).flatten()
    print(f"  â†’ é¢„æµ‹æ¦‚ç‡èŒƒå›´: [{y_pred_prob.min():.4f}, {y_pred_prob.max():.4f}]")
    print(f"  â†’ å¹³å‡é¢„æµ‹æ¦‚ç‡: {y_pred_prob.mean():.4f}")
    print(f"  â†’ é¢„æµ‹æ¦‚ç‡ä¸­ä½æ•°: {np.median(y_pred_prob):.4f}")
    
    # 5.1 å¯»æ‰¾æœ€ä½³é˜ˆå€¼
    print(f"\n  â†’ å¯»æ‰¾æœ€ä½³é¢„æµ‹é˜ˆå€¼...")
    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
    
    best_threshold = 0.5
    best_f1 = 0
    threshold_results = []
    
    for threshold in np.arange(0.1, 0.9, 0.05):
        y_pred_temp = (y_pred_prob > threshold).astype(int)
        f1_temp = f1_score(data['y_test'], y_pred_temp, zero_division=0)
        recall_temp = recall_score(data['y_test'], y_pred_temp, zero_division=0)
        precision_temp = precision_score(data['y_test'], y_pred_temp, zero_division=0)
        threshold_results.append({
            'threshold': threshold,
            'f1': f1_temp,
            'recall': recall_temp,
            'precision': precision_temp
        })
        if f1_temp > best_f1:
            best_f1 = f1_temp
            best_threshold = threshold
    
    print(f"  â†’ æœ€ä½³é˜ˆå€¼: {best_threshold:.2f} (F1={best_f1:.4f})")
    print(f"  â†’ ä½¿ç”¨æœ€ä½³é˜ˆå€¼è¿›è¡Œé¢„æµ‹...")
    
    y_pred = (y_pred_prob > best_threshold).astype(int)
    print(f"  â†’ é¢„æµ‹ä¸ºé«˜æ³¢åŠ¨çš„æ ·æœ¬æ•°: {y_pred.sum()}/{len(y_pred)} ({y_pred.sum()/len(y_pred)*100:.1f}%)")
    print(f"  â†’ å®é™…é«˜æ³¢åŠ¨çš„æ ·æœ¬æ•°: {data['y_test'].sum()}/{len(data['y_test'])} ({data['y_test'].sum()/len(data['y_test'])*100:.1f}%)")
    
    # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
    print(f"  â†’ è®¡ç®—æ€§èƒ½æŒ‡æ ‡...")
    accuracy = accuracy_score(data['y_test'], y_pred)
    precision = precision_score(data['y_test'], y_pred, zero_division=0)
    recall = recall_score(data['y_test'], y_pred, zero_division=0)
    f1 = f1_score(data['y_test'], y_pred, zero_division=0)
    
    # è®¡ç®—ROC-AUC
    try:
        roc_auc = roc_auc_score(data['y_test'], y_pred_prob)
        print(f"  â†’ ROC-AUCå¾—åˆ†: {roc_auc:.4f}")
    except:
        roc_auc = 0
        print(f"  â†’ ROC-AUCå¾—åˆ†: æ— æ³•è®¡ç®—")
    
    print("\n" + "="*70)
    print("æ¨¡å‹æ€§èƒ½è¯„ä¼°")
    print("="*70)
    print(f"æœ€ä½³é¢„æµ‹é˜ˆå€¼:       {best_threshold:.2f}   - ä¼˜åŒ–åçš„åˆ†ç±»é˜ˆå€¼")
    print(f"å‡†ç¡®ç‡ (Accuracy):  {accuracy*100:.2f}%  - é¢„æµ‹å¯¹çš„æ¯”ä¾‹")
    print(f"ç²¾ç¡®ç‡ (Precision): {precision*100:.2f}%  - é¢„è­¦å‡†ç¡®åº¦ï¼ˆé¢„è­¦æ—¶çœŸçš„é«˜æ³¢åŠ¨çš„æ¦‚ç‡ï¼‰")
    print(f"å¬å›ç‡ (Recall):    {recall*100:.2f}%  - æ•è·ç‡ï¼ˆé«˜æ³¢åŠ¨æ—¶èƒ½é¢„è­¦çš„æ¦‚ç‡ï¼‰")
    print(f"F1åˆ†æ•°:            {f1:.3f}     - ç»¼åˆæŒ‡æ ‡ï¼ˆç²¾ç¡®ç‡å’Œå¬å›ç‡çš„å¹³å‡ï¼‰")
    if roc_auc > 0:
        print(f"ROC-AUC:           {roc_auc:.3f}     - æ¨¡å‹æ•´ä½“åŒºåˆ†èƒ½åŠ›")
    
    # æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(data['y_test'], y_pred)
    print(f"\næ··æ·†çŸ©é˜µ:")
    print(f"              é¢„æµ‹ä½æ³¢åŠ¨  é¢„æµ‹é«˜æ³¢åŠ¨")
    print(f"å®é™…ä½æ³¢åŠ¨:      {cm[0,0]:4d}       {cm[0,1]:4d}")
    print(f"å®é™…é«˜æ³¢åŠ¨:      {cm[1,0]:4d}       {cm[1,1]:4d}")
    
    # 6. æ‰¾å‡ºæœ€è¿‘çš„é«˜æ³¢åŠ¨é¢„è­¦
    print("\n" + "="*70)
    print("æœ€è¿‘çš„é«˜æ³¢åŠ¨é¢„è­¦äº‹ä»¶")
    print("="*70)
    
    high_vol_indices = np.where(y_pred == 1)[0]
    if len(high_vol_indices) > 0:
        # æ˜¾ç¤ºæœ€è¿‘10ä¸ªé¢„è­¦
        recent_warnings = high_vol_indices[-10:] if len(high_vol_indices) >= 10 else high_vol_indices
        
        for idx in recent_warnings:
            date = data['test_dates'][idx]
            price = data['test_prices'][idx]
            prob = y_pred_prob[idx]
            actual = data['y_test'][idx]
            
            status = "âœ… æ­£ç¡®é¢„è­¦" if actual == 1 else "âŒ è¯¯æŠ¥"
            print(f"{date.strftime('%Y-%m-%d')} | ä»·æ ¼: ${price:,.2f} | é¢„è­¦æ¦‚ç‡: {prob*100:.1f}% | {status}")
    else:
        print("æµ‹è¯•é›†ä¸­æ²¡æœ‰é¢„è­¦é«˜æ³¢åŠ¨äº‹ä»¶")
    
    # 7. é¢„æµ‹æœªæ¥
    print("\n" + "="*70)
    print(f"æœªæ¥{DAYS_AHEAD}å¤©æ³¢åŠ¨é¢„è­¦")
    print("="*70)
    
    # ä½¿ç”¨æœ€æ–°æ•°æ®é¢„æµ‹
    print(f"  â†’ ä½¿ç”¨æœ€æ–°{TIME_STEPS}å¤©æ•°æ®è¿›è¡Œé¢„æµ‹...")
    latest_data = data['X_test'][-1:]
    print(f"  â†’ è¾“å…¥æ•°æ®å½¢çŠ¶: {latest_data.shape}")
    
    future_prob = model.predict(latest_data, verbose=0)[0][0]
    future_pred = 1 if future_prob > best_threshold else 0  # ä½¿ç”¨æœ€ä½³é˜ˆå€¼
    
    latest_date = data['test_dates'][-1]
    latest_price = data['test_prices'][-1]
    
    print(f"  â†’ é¢„æµ‹å®Œæˆ!")
    print(f"  â†’ ä½¿ç”¨é˜ˆå€¼: {best_threshold:.2f}")
    
    print(f"å½“å‰æ—¥æœŸ: {latest_date.strftime('%Y-%m-%d')}")
    print(f"å½“å‰ä»·æ ¼: ${latest_price:,.2f}")
    print(f"\næœªæ¥{DAYS_AHEAD}å¤©æ³¢åŠ¨é¢„æµ‹:")
    print(f"é«˜æ³¢åŠ¨æ¦‚ç‡: {future_prob*100:.1f}%")
    print(f"é¢„æµ‹é˜ˆå€¼: {best_threshold*100:.0f}%")
    
    if future_pred == 1:
        print(f"âš ï¸  é¢„è­¦ï¼šæœªæ¥{DAYS_AHEAD}å¤©å¯èƒ½å‡ºç°å¤§æ¶¨å¤§è·Œï¼ˆæ¶¨è·Œå¹…>Â±{THRESHOLD*100}%ï¼‰")
        print(f"å»ºè®®ï¼šæ³¨æ„é£é™©ï¼Œè€ƒè™‘æ­¢æŸæˆ–è§‚æœ›")
    else:
        print(f"âœ… æ­£å¸¸ï¼šæœªæ¥{DAYS_AHEAD}å¤©é¢„è®¡æ³¢åŠ¨è¾ƒå°")
        print(f"å»ºè®®ï¼šå¸‚åœºç›¸å¯¹ç¨³å®šï¼Œå¯æ­£å¸¸æ“ä½œ")
    
    # 8. å¯è§†åŒ–
    print("\nç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
    print(f"  â†’ åˆ›å»ºå›¾è¡¨ (20x12è‹±å¯¸)...")
    
    # å›¾1: æµ‹è¯•é›†é¢„æµ‹ç»“æœ
    plt.figure(figsize=(20, 12))
    
    # å­å›¾1: ä»·æ ¼å’Œæ³¢åŠ¨é¢„è­¦
    print(f"  â†’ ç»˜åˆ¶å­å›¾1: ä»·æ ¼ä¸æ³¢åŠ¨é¢„è­¦...")
    plt.subplot(3, 1, 1)
    plt.plot(data['test_dates'], data['test_prices'], label='ä»·æ ¼', linewidth=2, color='blue')
    
    # æ ‡è®°å®é™…é«˜æ³¢åŠ¨ç‚¹
    actual_high_vol = np.where(data['y_test'] == 1)[0]
    if len(actual_high_vol) > 0:
        print(f"  â†’ æ ‡è®°{len(actual_high_vol)}ä¸ªå®é™…é«˜æ³¢åŠ¨ç‚¹...")
        plt.scatter(data['test_dates'][actual_high_vol], 
                   data['test_prices'][actual_high_vol],
                   color='red', s=100, marker='x', label='å®é™…é«˜æ³¢åŠ¨', zorder=5)
    
    # æ ‡è®°é¢„æµ‹é«˜æ³¢åŠ¨ç‚¹
    pred_high_vol = np.where(y_pred == 1)[0]
    if len(pred_high_vol) > 0:
        print(f"  â†’ æ ‡è®°{len(pred_high_vol)}ä¸ªé¢„æµ‹é«˜æ³¢åŠ¨ç‚¹...")
        plt.scatter(data['test_dates'][pred_high_vol], 
                   data['test_prices'][pred_high_vol],
                   color='orange', s=100, marker='o', alpha=0.5, label='é¢„æµ‹é«˜æ³¢åŠ¨', zorder=4)
    
    plt.title(f'æ¯”ç‰¹å¸ä»·æ ¼ä¸æ³¢åŠ¨é¢„è­¦ (é˜ˆå€¼ï¼š{THRESHOLD*100}%)', fontsize=16)
    plt.ylabel('ä»·æ ¼ (ç¾å…ƒ)', fontsize=12)
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # å­å›¾2: é¢„æµ‹æ¦‚ç‡æ›²çº¿
    print(f"  â†’ ç»˜åˆ¶å­å›¾2: é¢„æµ‹æ¦‚ç‡æ›²çº¿...")
    plt.subplot(3, 1, 2)
    plt.plot(data['test_dates'], y_pred_prob, label='é«˜æ³¢åŠ¨æ¦‚ç‡', linewidth=2, color='purple')
    plt.axhline(y=best_threshold, color='red', linestyle='--', alpha=0.7, 
                label=f'æœ€ä½³é˜ˆå€¼({best_threshold:.2f})', linewidth=2)
    plt.axhline(y=0.5, color='orange', linestyle=':', alpha=0.5, label='é»˜è®¤é˜ˆå€¼(0.50)')
    plt.fill_between(data['test_dates'], 0, y_pred_prob, 
                     where=(y_pred_prob > best_threshold), alpha=0.3, color='red', label='é«˜æ³¢åŠ¨åŒº')
    plt.title(f'é«˜æ³¢åŠ¨é¢„æµ‹æ¦‚ç‡ï¼ˆä¼˜åŒ–é˜ˆå€¼ï¼š{best_threshold:.2f}ï¼‰', fontsize=16)
    plt.ylabel('æ¦‚ç‡', fontsize=12)
    plt.ylim(0, 1)
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # å­å›¾3: æ··æ·†çŸ©é˜µ
    print(f"  â†’ ç»˜åˆ¶å­å›¾3: æ··æ·†çŸ©é˜µ...")
    plt.subplot(3, 1, 3)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=['é¢„æµ‹ä½æ³¢åŠ¨', 'é¢„æµ‹é«˜æ³¢åŠ¨'],
                yticklabels=['å®é™…ä½æ³¢åŠ¨', 'å®é™…é«˜æ³¢åŠ¨'])
    plt.title('æ··æ·†çŸ©é˜µ', fontsize=16)
    
    print(f"  â†’ è°ƒæ•´å¸ƒå±€å¹¶ä¿å­˜...")
    plt.tight_layout()
    plt.savefig('æ³¢åŠ¨é¢„è­¦ç»“æœ.png', dpi=150)
    print("âœ“ ä¿å­˜å›¾è¡¨: æ³¢åŠ¨é¢„è­¦ç»“æœ.png")
    
    # ä¿å­˜é¢„è­¦è®°å½•
    if len(high_vol_indices) > 0:
        print(f"  â†’ ç”Ÿæˆé¢„è­¦è®°å½•CSVæ–‡ä»¶...")
        warnings_df = pd.DataFrame({
            'æ—¥æœŸ': [data['test_dates'][i].strftime('%Y-%m-%d') for i in high_vol_indices],
            'ä»·æ ¼': [data['test_prices'][i] for i in high_vol_indices],
            'é¢„è­¦æ¦‚ç‡': [y_pred_prob[i] for i in high_vol_indices],
            'å®é™…æ³¢åŠ¨': ['é«˜æ³¢åŠ¨' if data['y_test'][i] == 1 else 'ä½æ³¢åŠ¨' for i in high_vol_indices],
            'é¢„è­¦ç»“æœ': ['æ­£ç¡®' if data['y_test'][i] == 1 else 'è¯¯æŠ¥' for i in high_vol_indices]
        })
        print(f"  â†’ é¢„è­¦è®°å½•æ•°é‡: {len(warnings_df)}")
        warnings_df.to_csv('æ³¢åŠ¨é¢„è­¦è®°å½•.csv', index=False, encoding='utf-8-sig')
        print("âœ“ ä¿å­˜æ–‡ä»¶: æ³¢åŠ¨é¢„è­¦è®°å½•.csv")
    
    print("\n" + "="*70)
    print("æ‰€æœ‰ä»»åŠ¡å®Œæˆ!")
    print("="*70)
    
    # è¿”å›ç»“æœä¾›è¿›ä¸€æ­¥åˆ†æ
    return {
        'model': model,
        'data': data,
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'roc_auc': roc_auc,
        'best_threshold': best_threshold,
        'future_prob': future_prob,
        'class_weight_dict': class_weight_dict,
        'threshold_results': threshold_results
    }

if __name__ == "__main__":
    results = main()

